{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4666c7c-3d04-4a07-b993-e69056d3f72d",
   "metadata": {},
   "source": [
    "## Tarea 3\n",
    "## Jose Juan Bastida Perez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69167ec-282f-40fe-9163-06d02b11ae57",
   "metadata": {},
   "source": [
    "\n",
    "**Importamos las librerias necesarias**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c22a2-d94d-4fcb-9f01-416eafeb1cba",
   "metadata": {},
   "source": [
    "Pasos previos:\n",
    "* Clonamos el repositorio con el comando git clone\n",
    "* Creamos el entorno de conda con el comando conda create --name= seguido del nombre del archivo que vamos a usar para definir el entorno y lo activamos\n",
    "* Una vez esta activado podemos lanzar jupyter-lab y creamos nuestro notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5af1c7-da62-4ea1-b919-3ddb7ed50ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import IPython\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f46b89d-6188-466d-8b15-d9ae2eeb7b11",
   "metadata": {},
   "source": [
    "**Ahora tenemos que definir donde guardaremos los audios o de donde los cogeremos**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260eee17-92e5-4786-a770-b7ce8fa9716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()  #guardamos en cwd la ruta actual\n",
    "audio_input_path = os.path.join(cwd, os.path.join('audio', 'ejemplos'))  \n",
    "audio_output_path = os.path.join(cwd, os.path.join('audio', 'output'))\n",
    "\n",
    "print(f'Directorio con los audios de entrada: {audio_input_path}')\n",
    "print(f'Directorio donde guardaremos los audios generados: {audio_output_path}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7974eae4-f31e-471f-90c9-7137475939d9",
   "metadata": {},
   "source": [
    "**Ahora vamos a cargar un archivo de audio de la carpeta ejemplos**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe671aba-1d74-4063-a129-1b8da2f86065",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = os.path.join(audio_input_path,'breaking_bad.wav')\n",
    "sample_rate, audio_data = wavfile.read(filename)  #en sample rate se guardara la frecuencia y en audio data los datos de la cancion\n",
    "print(f'Frecuencia de muestreo (sample rate): {sample_rate/1000} kHz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6220c2f-10ff-4419-bd9c-4ddaecc04551",
   "metadata": {},
   "source": [
    "**Mostramos la frecuencia de muestreo y en audio data tenemos la información de la cancion**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db111d0a-e63d-49a8-8d38-94d5539f8de9",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "\n",
    "**Para reproducirlo usamos la funcion display.Audio donde es necesario pasarle el audio data y la frecuencia de muestreo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa9d3b-1d57-4c89-b482-7fb4333c97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(audio_data.T, rate=sample_rate) # .T se pasa únicamente si es audio estéreo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae8225-8e70-4aaf-87ed-0b65f034b0a0",
   "metadata": {},
   "source": [
    "**A continuacion vamos a mostrar la informacion principal de la onda, la cual la tenemos en audio_data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2618d-0bdb-43d0-9422-a9413d278c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Datos de audio (estereo):')\n",
    "print(f'- Tamaño:     {audio_data.shape}') #aqui se enseña el numero de muestras y el numero de canales\n",
    "print(f'- 1º canal:   {audio_data[:10, 0]}...') #aqui mostramos 5 elementos del primer canal\n",
    "print(f'- 2º canal:   {audio_data[:10, 1]}...') #aqui mostramos 5 elementos del primer canal\n",
    "print(f'- Resolucion: {type(audio_data[0,0])}\\n') #mostramos el tipo de dato"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13470ea-3cc8-4310-8037-347675b13a6a",
   "metadata": {},
   "source": [
    "**Para pasar un audio a mono, lo que tendriamos que hacer seria juntar los sonidos en un mismo canal, por lo tanto calculamos la media de ambos canales**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d8f79-ebeb-4282-938a-1519a43f124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convertimos a mono mediante la media por canal (simplificacion).\n",
    "new_data_mono = audio_data.mean(axis=1)  # Se hace la media de dos muestras, de ambos canales\n",
    "print('Nuevos datos de audio (mono):')\n",
    "print(f'- Nuevo tamaño: {new_data_mono.shape}')  #aqui se enseña el numero de muestras y el numero de canales (en este caso solo hay 1)\n",
    "print(f'- Canal unico:  {new_data_mono[:5]}...') #muestra 5 elementos del canal unico\n",
    "\n",
    "# Mantenemos la misma resolucion que antes y la mostramos para comprobar que este correcta.\n",
    "new_data_mono = new_data_mono.astype(np.int16)\n",
    "print(f'- Resolucion:   {type(new_data_mono[0])}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688153f5-45cc-4bfd-a775-aa6c693b2e20",
   "metadata": {},
   "source": [
    "**Ahora procedemos a guardar el archivo en la ruta que nos indica audio_output_path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61621f81-bf9c-408c-90e6-17253fba39d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el archivo mono a un fichero de tipo wav.\n",
    "wavfile.write(\n",
    "    filename=os.path.join(audio_output_path, 'ejemplo_mono.wav'),\n",
    "    rate=sample_rate,\n",
    "    data=new_data_mono\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612fe4ac-3dfd-41c9-a2eb-0a0312721844",
   "metadata": {},
   "source": [
    "**Volvemos a escucharlo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0964e075-deb2-46ad-9208-e03dedc76661",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(new_data_mono, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003fd52-7a0e-4986-a889-d42e7d30ad45",
   "metadata": {},
   "source": [
    "**Grafica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0b2ab-a689-4485-9221-90b5ca606feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos de audio estéreo y mono (asumiendo que ya has cargado los datos)\n",
    "# audio_data_stereo y audio_data_mono son los arrays NumPy que contienen los datos de audio\n",
    "\n",
    "# Configuración del tamaño de la figura\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Graficar audio estéreo\n",
    "plt.subplot(2, 1, 1)  # Configurar subplot para el audio estéreo\n",
    "plt.plot(np.arange(len(audio_data)) / sample_rate, audio_data[:, 0], label='Canal izquierdo')\n",
    "plt.plot(np.arange(len(audio_data)) / sample_rate, audio_data[:, 1], label='Canal derecho')\n",
    "plt.xlabel('Tiempo (s)')\n",
    "plt.ylabel('Amplitud')\n",
    "plt.title('Audio Estéreo')\n",
    "plt.legend()\n",
    "\n",
    "# Graficar audio mono\n",
    "plt.subplot(2, 1, 2)  # Configurar subplot para el audio mono\n",
    "plt.plot(np.arange(len(new_data_mono)) / sample_rate, new_data_mono, color='red')\n",
    "plt.xlabel('Tiempo (s)')\n",
    "plt.ylabel('Amplitud')\n",
    "plt.title('Audio Mono')\n",
    "\n",
    "# Ajustar diseño y mostrar la gráfica\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf49fa3-1f7d-4630-8035-ad3284ebe414",
   "metadata": {},
   "source": [
    "**La principal diferencia entre el sonido es que en estereo se reparte entre los dos canales de sonido, mientras que el sonido en mono solo va por un unico canal.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d40e9-a1fb-4722-844f-bda7385bef17",
   "metadata": {},
   "source": [
    "## Tarea 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e767d8c-f781-46c9-879b-3c6cf556acd8",
   "metadata": {},
   "source": [
    "\n",
    "**Primero añadimos las librerias necesarias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c17a2f-25a6-4fd5-b83b-b5dfe90a61ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import IPython\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b011b63-80b7-4b1e-8db7-4ba45ceafc6d",
   "metadata": {},
   "source": [
    "*  **Frecuencia de muestreo** : Es el numero de muestras o de veces que tomamos informacion por segundo, por lo que tambien seria, la velocidad con lo que lo hacemos. Cuanto mayor es la frecuencia de muestreo podriamos decir que mayor es la calidad y la resolucion de esta(aunque esto haga que el archivo aumente su peso consideradamente).\n",
    "* **Aliasing**: Es el efecto sobre las señales continuas que hace que por una baja frecuencia de muestreo, no podamos distinguirla bien respecto a otras cuando son muestreadas digitalmente. Esto se produce por tomas un numero de muestras tan bajo que no tengamos una suficiente informacion acerca de dicha onda y su representacion no da la suficiente informacion.\n",
    "* **Profundidad de bits**: Nos dice cuantos bits tenemos disponibles para medir la onda.\n",
    "* **Ancho de banda**: Es la suma de profundidad de bits + la frecuencia de muestreo. Este seria el rango de frecuencias en las que se trabaja, de la mayor a la menor.\n",
    "* **Tasa de bits**: Nos indica el numero de bits por segundo que deberiamos procesar para que este se escuche de la forma prevista, esto lo hace multiplicando la frecuencia de muestreo, la profundidad de bits y el numero de canales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121d81d-2ba4-4135-a02a-c75e3a9655c7",
   "metadata": {},
   "source": [
    "**Fournier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37109954-2728-4bec-a0b4-3954c54308fd",
   "metadata": {},
   "source": [
    "**Primero cargamos un audio mono**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f228d93f-b672-44c9-b84e-f62847ef8d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audio_input_path = os.path.join(cwd, os.path.join('audio', 'output'))  \n",
    "frecuencia, audio_mono_data = wavfile.read(filename=os.path.join(audio_input_path, 'ejemplo_mono.wav'))\n",
    "print(f'{audio_mono_data.shape}\\n') \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7fb38-0403-43bf-94b6-4ef2ff9bcbf2",
   "metadata": {},
   "source": [
    "**Ahora vamos a aplicar la transformada de fourier, es decir, descomponer la señal en vez de en el dominio del tiempo en el de la frecuencia**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1790465-6bad-4df1-a644-8f17ae5e4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = len(audio_mono_data)  #tamaño del array de los datos\n",
    "Fs = frecuencia   #frecuencia de muestreo\n",
    "\n",
    "# Working with stereo audio, there are two channels in the audio data.\n",
    "# Let's retrieve each channel seperately:\n",
    "# ch1 = np.array([data[i][0] for i in range(n)]) #channel 1\n",
    "# ch2 = np.array([data[i][1] for i in range(n)]) #channel 2\n",
    "# We can then perform a Fourier analysis on the first\n",
    "# channel to see what the spectrum looks like.\n",
    "\n",
    "# Calculando la Transformada Rapida de Fourier (FFT) en audio mono (si fuese en estereo tendriamos dos canales con los que trabajar como se ve arriba).\n",
    "ch_Fourier = np.fft.fft(audio_mono_data) \n",
    "\n",
    "# Solo miramos frecuencia por debajo de Fs/2\n",
    "# (Nyquist-Shannon) --> Spectrum.\n",
    "abs_ch_Fourier = np.absolute(ch_Fourier[:n//2]) #guardamos en esta variable el valor absoluto de la transformada de fourier, pero se toma solo la mitad hasta n//2 , es decir, solo considera las fracuencias positivas\n",
    "\n",
    "# Graficamos.\n",
    "plt.plot(np.linspace(0, Fs/2, n//2), abs_ch_Fourier) #Traza un grafico de la amplitud de las frecuencias frente a las frecuencias correspondientes\n",
    "plt.ylabel('Amplitud', labelpad=10) #etiqueta\n",
    "plt.xlabel('$f$ (Hz)', labelpad=10) #etiqueta\n",
    "plt.show() #muestra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dda960a-92cb-430c-b775-95b9fbc733af",
   "metadata": {},
   "source": [
    "**Ahora vamos a calcular la energia del espectograma y la frecuencia de corte**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52072a-3109-4be0-8346-e3116fee7ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eps = [1e-5, .02, .041, .063, .086, .101, .123] #Definimos una serie de valores para epsilon, que es el valor que determina la energia que se conservara al aplicar el umbral\n",
    "\n",
    "eps = eps[3] #aqui marcamos el que queramos usar\n",
    "print(f'Epsilon: {eps}')\n",
    "\n",
    "thr_spec_energy = (1 - eps) * np.sum(abs_ch_Fourier) #Guardamos el valor del umbral que se utilizara para determinar que frecuencias se conservaran\n",
    "print(f'Valor de corte para la energia del espectro: {thr_spec_energy}')\n",
    "\n",
    "spec_energy = np.cumsum(abs_ch_Fourier) #Aqui se calcula la energía acumulada del espectro,es decir, se suma la energía de cada frecuencia en el espectro para cada punto de frecuencia.\n",
    "\n",
    "frequencies_to_remove = thr_spec_energy < spec_energy #Aqui estamos creando una mascara que nos servira para determinar las frecuencias a eliminar del espectro.\n",
    "print(f'Mascara: {frequencies_to_remove}')\n",
    "\n",
    "f0 = (len(frequencies_to_remove) - np.sum(frequencies_to_remove)) * (Fs/2) / (n//2) #Es la frecuencia mas alta en el espectro que se conservara después de aplicar el umbral, es decir, el umbral.\n",
    "print(f'Frecuencia de corte f0 (Hz): {int(f0)}')\n",
    "\n",
    "# Graficamos.\n",
    "plt.axvline(f0, color='r')\n",
    "plt.plot(np.linspace(0, Fs/2, n//2), abs_ch_Fourier)\n",
    "plt.ylabel('Amplitud')\n",
    "plt.xlabel('$f$ (Hz)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f930791b-7981-4aa9-ab79-54f86cea7910",
   "metadata": {},
   "source": [
    "**Aqui podemos ver la salida eligiendo el de la posicion [3] y que el valor de epsilon seria de 0.063. La frecuencia de corte seria 9643 Hz, que vemos que quedaria sobre la mitad de la grafica**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89927bab-4fc7-4767-8257-b79ca291a056",
   "metadata": {},
   "source": [
    "## Compresion del archivo ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187a518c-25c2-4890-af7c-74a2bace89ca",
   "metadata": {},
   "source": [
    "**Para comprimir el archivo vamos a quedarnos con menos valores del audio original e intentando perder la menor informacion importante necesaria. Los elementos que vamos a seleccionar serias aquellos que esten en posicion multiplo del valor D(slicing). D=fs/f0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763fac40-5c29-41e7-a48b-c495b0142f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_input_path = os.path.join(cwd, os.path.join('audio', 'output'))  \n",
    "frecuencia, audio_mono_data = wavfile.read(filename=os.path.join(audio_input_path, 'ejemplo_mono.wav'))\n",
    "print(f'{audio_mono_data.shape}\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b0ee0-447e-4e26-80ce-81497733a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_compressed_file = \"audio_mono_comp.wav\" # Indicamos el nombre del archivo y lo almacenamos en una variable\n",
    "\n",
    "D = int(Fs / f0) #obtenemos el valor D\n",
    "print(f'Factor de downsampling: {D}') #muestra\n",
    "\n",
    "# #Se utiliza la tecnica de slicing creando nuevos conjuntos de datos con parte de los datos del original (los que estan en las posiciones que estan marcadas por el valor D)\n",
    "new_data = audio_mono_data[::D] \n",
    "\n",
    "# Escribimos los datos a un archivo de tipo wav.\n",
    "wavfile.write(\n",
    "    filename=os.path.join(audio_output_path, wav_compressed_file),\n",
    "    rate=int(Fs/D),\n",
    "    data=new_data\n",
    ")\n",
    "\n",
    "\n",
    "# Cargamos el nuevo archivo.\n",
    "new_sample_rate, new_audio_data = wavfile.read(filename=os.path.join(audio_output_path, wav_compressed_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859dbf6-6a78-4869-a9df-46332990a2fd",
   "metadata": {},
   "source": [
    "## Espectrograma ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65fefb-356a-4473-8f87-ad559b8748ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "#Se calcula y se traza es espectrograma con el primer valor, ax[0], es decor en el primer subplot\n",
    "Pxx, freqs, bins, im = ax[0].specgram(audio_mono_data, NFFT=1024, Fs=frecuencia, noverlap=512) \n",
    "ax[0].set_title('Espectograma del audio original')\n",
    "ax[0].set_ylabel('Frecuencia (Hz)')\n",
    "ax[0].grid(True)\n",
    "\n",
    "#Se calcula y se traza es espectrograma con el segundo valor, ax[1], es decir en el segundo subplot\n",
    "Pxx, freqs, bins, im = ax[1].specgram(new_audio_data, NFFT=1024, Fs=new_sample_rate, noverlap=512)\n",
    "ax[1].set_title('Espectrograma del audio reducido/comprimido')\n",
    "ax[1].set_xlabel('Tiempo (s)')\n",
    "ax[1].set_ylabel('Frecuencia (Hz)')\n",
    "ax[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d8a61e-2ca0-470f-87ac-d64cb11c5fc8",
   "metadata": {},
   "source": [
    "**Comprobamos los tamaños de ambos archivos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ede01c-99ef-4607-bfcc-dfd479203e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -sh /home/usuario/Descargas/P2SM/audio/output/ejemplo_mono.wav\n",
    "!ls -sh /home/usuario/P2SM/audio/output/ejemplo_mono_comp.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3f814-b241-40e7-a855-8eca0abace7f",
   "metadata": {},
   "source": [
    "**Por ultimo escuchamos ambos audios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3425ac4-1b58-4c09-ab1b-93669313ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(audio_mono_data, rate=frecuencia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b97ea-c973-4413-ba6f-600c5501adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(new_audio_data, rate=Fs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condita",
   "language": "python",
   "name": "condita"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
